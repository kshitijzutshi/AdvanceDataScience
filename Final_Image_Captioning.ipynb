{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "image-caption-generator-with-cnn-lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahparth0007/AdvanceDataScience/blob/main/Final_Image_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abstract\n",
        "\n",
        "Image caption generation/ image summarization is a task that involves generating a semantic description of an image in natural language and is currently accomplished by techniques that use a combination of computer vision (CV), natural language processing (NLP), and machine learning methods. The inspiration for such an application can be inferred from Social media platforms like Facebook(Now Meta), that summarize the image posted by the user and infer details like - where you are, what you wear etc. This application also has a profound use in assisting visually impared individuals in comprehending the images of the real world. In this task, we work on a model that generates natural language description of an image. We intend to use a combination of convolutional neural networks to extract features and then use recurrent neural networks to generate text from these features. We incorporated the attention mechanism while generating captions. We evaluated the model on the Flikr8k database.bold text"
      ],
      "metadata": {
        "id": "-6fuvNFQ-vsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://user-images.githubusercontent.com/91229784/166126929-d8c0483d-3e88-4cdd-b194-299b3e69e43e.png)\n"
      ],
      "metadata": {
        "id": "g3QRdq8__I8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n",
        "Given an image, we want to obtain a sentence that describes what the image consists of."
      ],
      "metadata": {
        "id": "h-Qhpcx4Cx1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: We suggest you to start GPU for this session as this is a image summarization model.\n",
        "\n",
        "***Steps:***\n",
        "\n",
        "*   On the header click on \"Runtime\"\n",
        "*   Click on \"Change runtime type\"\n",
        "*   Click on \"Hardware accelerator\"\n",
        "*   Select \"GPU\"\n",
        "*   Click on \"Save\"\n",
        "\n",
        "And you are all set to do image caption generation"
      ],
      "metadata": {
        "id": "OjacSIqgEyqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1) Importing Libraries**"
      ],
      "metadata": {
        "id": "ueWCGmbPDPhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import plotly.express as px\n",
        "\n",
        "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add, GRU"
      ],
      "metadata": {
        "id": "EJSpc4uK5CnC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2) Setup Data**\n",
        "We have made arrangements where you can directly load the data from our kaggle accounts!\n",
        "\n",
        "\n",
        "\n",
        "1.   First with the help of gdown the Kaggle.json file will be downloaded\n",
        "2.   After which we will install kaggle library and get the data from kaggle \n",
        "\n",
        "Things which will get downloaded:\n",
        "\n",
        "\n",
        "*   The flicker3k data with captions\n",
        "*   Our trained Model V1 (With parameters: )\n",
        "*   Our trained Model V2 (With parameters: )\n",
        "*   The features.pkl file. (This file is the output of feature generation model VGG)\n",
        "\n",
        "All of the above files can be seen in the contents of your google collab session!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "unk4w0xzDdkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1jXH8vzpVy4gAkc-1lrWlwVUNF4w0B0dU\n",
        "!pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download parthshah0007/flickerdataset\n",
        "! unzip -q '/content/flickerdataset.zip' -d '/content/flickr8k/'"
      ],
      "metadata": {
        "id": "HyAqjKs1olWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae3fc4d4-293c-4644-f192-65958b3ebe1a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1jXH8vzpVy4gAkc-1lrWlwVUNF4w0B0dU\n",
            "To: /content/kaggle.json\n",
            "100% 69.0/69.0 [00:00<00:00, 115kB/s]\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "404 - Not Found\n",
            "unzip:  cannot find or open /content/flickerdataset.zip, /content/flickerdataset.zip.zip or /content/flickerdataset.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = '/content/flickr8k/FlickerAllData/Data/'\n",
        "WORKING_DIR = '/content/flickr8k/FlickerAllData/'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-19T23:53:58.186662Z",
          "iopub.execute_input": "2022-04-19T23:53:58.18708Z",
          "iopub.status.idle": "2022-04-19T23:53:58.198141Z",
          "shell.execute_reply.started": "2022-04-19T23:53:58.187043Z",
          "shell.execute_reply": "2022-04-19T23:53:58.197156Z"
        },
        "trusted": true,
        "id": "FqxE6nPyji4K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3) Loading the VGG pretrained Model**"
      ],
      "metadata": {
        "id": "2O4in1UUGOD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load vgg19 model\n",
        "model = VGG19()\n",
        "# restructure the model\n",
        "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "# summarize\n",
        "print(model.summary())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-19T22:28:44.433752Z",
          "iopub.execute_input": "2022-04-19T22:28:44.434651Z",
          "iopub.status.idle": "2022-04-19T22:28:51.016823Z",
          "shell.execute_reply.started": "2022-04-19T22:28:44.434611Z",
          "shell.execute_reply": "2022-04-19T22:28:51.016094Z"
        },
        "trusted": true,
        "id": "GQpIkjsrji4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4) Image Feature Extraction (Commented)**\n",
        "\n",
        "Using the VGG pretrained model we will extract the features of the images.\n",
        "\n",
        "More About VGG can be found: https://keras.io/api/applications/vgg/\n",
        "\n",
        "Note: We have commented this as this code takes time to run!\n",
        "We have already created the features and stored in features.pkl file which we downloaded from kaggle"
      ],
      "metadata": {
        "id": "g55QWeyrGUB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # extract features from image\n",
        "\n",
        "# features = {}\n",
        "# directory = os.path.join(BASE_DIR, 'Images')\n",
        "\n",
        "# for img_name in tqdm(os.listdir(directory)):\n",
        "#     # load the image from file\n",
        "#     img_path = directory + '/' + img_name\n",
        "#     image = load_img(img_path, target_size=(224, 224))\n",
        "#     # convert image pixels to numpy array\n",
        "#     image = img_to_array(image)\n",
        "#     # reshape data for model\n",
        "#     image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "#     # preprocess image for vgg\n",
        "#     image = preprocess_input(image)\n",
        "#     # extract features\n",
        "#     feature = model.predict(image, verbose=0)\n",
        "#     # get image ID\n",
        "#     image_id = img_name.split('.')[0]\n",
        "#     # store feature\n",
        "#     features[image_id] = feature\n",
        "\n",
        "# # store features in pickle\n",
        "# pickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-19T22:28:54.090966Z",
          "iopub.execute_input": "2022-04-19T22:28:54.091232Z",
          "iopub.status.idle": "2022-04-19T22:36:59.961687Z",
          "shell.execute_reply.started": "2022-04-19T22:28:54.091204Z",
          "shell.execute_reply": "2022-04-19T22:36:59.961014Z"
        },
        "trusted": true,
        "id": "V5YNChB2ji4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load features from pickle\n",
        "with open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:\n",
        "    features = pickle.load(f)\n",
        "print(len(features))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-19T23:54:03.947172Z",
          "iopub.execute_input": "2022-04-19T23:54:03.948011Z",
          "iopub.status.idle": "2022-04-19T23:54:04.388607Z",
          "shell.execute_reply.started": "2022-04-19T23:54:03.947968Z",
          "shell.execute_reply": "2022-04-19T23:54:04.38779Z"
        },
        "trusted": true,
        "id": "ByYt3lsUji4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(BASE_DIR, 'captions.txt'), 'r') as f:\n",
        "    next(f)\n",
        "    captions_doc = f.read()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-19T23:54:06.213128Z",
          "iopub.execute_input": "2022-04-19T23:54:06.213873Z",
          "iopub.status.idle": "2022-04-19T23:54:06.26308Z",
          "shell.execute_reply.started": "2022-04-19T23:54:06.213824Z",
          "shell.execute_reply": "2022-04-19T23:54:06.26229Z"
        },
        "trusted": true,
        "id": "40Ke-xUZji4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create mapping of image to captions\n",
        "mapping = {}\n",
        "# process lines\n",
        "for line in tqdm(captions_doc.split('\\n')):\n",
        "    # split the line by comma(,)\n",
        "    tokens = line.split(',')\n",
        "    if len(line) < 2:\n",
        "        continue\n",
        "    image_id, caption = tokens[0], tokens[1:]\n",
        "    # remove extension from image ID\n",
        "    image_id = image_id.split('.')[0]\n",
        "    # convert caption list to string\n",
        "    caption = \" \".join(caption)\n",
        "    # create list if needed\n",
        "    if image_id not in mapping:\n",
        "        mapping[image_id] = []\n",
        "    # store the caption\n",
        "    mapping[image_id].append(caption)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-19T23:54:22.200081Z",
          "iopub.execute_input": "2022-04-19T23:54:22.2004Z",
          "iopub.status.idle": "2022-04-19T23:54:22.34996Z",
          "shell.execute_reply.started": "2022-04-19T23:54:22.200366Z",
          "shell.execute_reply": "2022-04-19T23:54:22.349241Z"
        },
        "trusted": true,
        "id": "6VCtAQFTji4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(mapping):\n",
        "    for key, captions in mapping.items():\n",
        "        for i in range(len(captions)):\n",
        "            # take one caption at a time\n",
        "            caption = captions[i]\n",
        "            # preprocessing steps\n",
        "            # convert to lowercase\n",
        "            caption = caption.lower()\n",
        "            # delete digits, special chars, etc., \n",
        "            caption = caption.replace('[^A-Za-z]', '')\n",
        "            # delete additional spaces\n",
        "            caption = caption.replace('\\s+', ' ')\n",
        "            # add start and end tags to the caption\n",
        "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n",
        "            captions[i] = caption"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-19T23:54:44.56188Z",
          "iopub.execute_input": "2022-04-19T23:54:44.564811Z",
          "iopub.status.idle": "2022-04-19T23:54:44.575492Z",
          "shell.execute_reply.started": "2022-04-19T23:54:44.564738Z",
          "shell.execute_reply": "2022-04-19T23:54:44.574598Z"
        },
        "trusted": true,
        "id": "RNGQkZnIji4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# before preprocess of text\n",
        "mapping['1000268201_693b08cb0e']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-19T23:54:46.251167Z",
          "iopub.execute_input": "2022-04-19T23:54:46.251845Z",
          "iopub.status.idle": "2022-04-19T23:54:46.257714Z",
          "shell.execute_reply.started": "2022-04-19T23:54:46.251801Z",
          "shell.execute_reply": "2022-04-19T23:54:46.256855Z"
        },
        "trusted": true,
        "id": "pEfLK3AZji4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess the text\n",
        "clean(mapping)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-19T23:54:49.033891Z",
          "iopub.execute_input": "2022-04-19T23:54:49.034567Z",
          "iopub.status.idle": "2022-04-19T23:54:49.168102Z",
          "shell.execute_reply.started": "2022-04-19T23:54:49.034529Z",
          "shell.execute_reply": "2022-04-19T23:54:49.167274Z"
        },
        "trusted": true,
        "id": "Fv10EdiBji4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after preprocess of text\n",
        "mapping['1000268201_693b08cb0e']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-19T23:54:49.98712Z",
          "iopub.execute_input": "2022-04-19T23:54:49.987701Z",
          "iopub.status.idle": "2022-04-19T23:54:49.992774Z",
          "shell.execute_reply.started": "2022-04-19T23:54:49.987659Z",
          "shell.execute_reply": "2022-04-19T23:54:49.992053Z"
        },
        "trusted": true,
        "id": "k8FM4vxrji4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_captions = []\n",
        "for key in mapping:\n",
        "    for caption in mapping[key]:\n",
        "        all_captions.append(caption)\n",
        "\n",
        "print(\"There are total of:\",len(all_captions),\" captions\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-19T23:54:59.778047Z",
          "iopub.execute_input": "2022-04-19T23:54:59.778666Z",
          "iopub.status.idle": "2022-04-19T23:54:59.791296Z",
          "shell.execute_reply.started": "2022-04-19T23:54:59.778625Z",
          "shell.execute_reply": "2022-04-19T23:54:59.790568Z"
        },
        "trusted": true,
        "id": "IW3QaEcdji4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-19T23:55:05.352201Z",
          "iopub.execute_input": "2022-04-19T23:55:05.352986Z",
          "iopub.status.idle": "2022-04-19T23:55:05.975781Z",
          "shell.execute_reply.started": "2022-04-19T23:55:05.352945Z",
          "shell.execute_reply": "2022-04-19T23:55:05.974894Z"
        },
        "trusted": true,
        "id": "cO_ArLh2ji4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get maximum length of the caption available\n",
        "max_length = max(len(caption.split()) for caption in all_captions)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-19T23:55:07.434534Z",
          "iopub.execute_input": "2022-04-19T23:55:07.435076Z",
          "iopub.status.idle": "2022-04-19T23:55:07.47275Z",
          "shell.execute_reply.started": "2022-04-19T23:55:07.435035Z",
          "shell.execute_reply": "2022-04-19T23:55:07.472034Z"
        },
        "trusted": true,
        "id": "w46OSNfVji4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_ids = list(mapping.keys())\n",
        "split = int(len(image_ids) * 0.90)\n",
        "train = image_ids[:split]\n",
        "test = image_ids[split:]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-19T23:55:09.307981Z",
          "iopub.execute_input": "2022-04-19T23:55:09.308297Z",
          "iopub.status.idle": "2022-04-19T23:55:09.313056Z",
          "shell.execute_reply.started": "2022-04-19T23:55:09.308245Z",
          "shell.execute_reply": "2022-04-19T23:55:09.312328Z"
        },
        "trusted": true,
        "id": "7a0ZTyuBji4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create data generator to get data in batch (avoids session crash)\n",
        "\n",
        "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
        "    # loop over images\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    n = 0\n",
        "    while 1:\n",
        "        for key in data_keys:\n",
        "            n += 1\n",
        "            captions = mapping[key]\n",
        "            # process each caption\n",
        "            for caption in captions:\n",
        "                # encode the sequence\n",
        "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "                # split the sequence into X, y pairs\n",
        "                for i in range(1, len(seq)):\n",
        "                    # split into input and output pairs\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "                    # pad input sequence\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    # encode output sequence\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                    \n",
        "                    # store the sequences\n",
        "                    X1.append(features[key][0])\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "            if n == batch_size:\n",
        "                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
        "                yield [X1, X2], y\n",
        "                X1, X2, y = list(), list(), list()\n",
        "                n = 0"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-19T23:55:15.141974Z",
          "iopub.execute_input": "2022-04-19T23:55:15.142279Z",
          "iopub.status.idle": "2022-04-19T23:55:15.151636Z",
          "shell.execute_reply.started": "2022-04-19T23:55:15.142233Z",
          "shell.execute_reply": "2022-04-19T23:55:15.150684Z"
        },
        "trusted": true,
        "id": "Iub_deu8ji4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 1: Long Short Term Memory**\n",
        "\n",
        "LSTM has three gates that are input, output, forget."
      ],
      "metadata": {
        "id": "9I7x3plb89oT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder model\n",
        "# image feature layers\n",
        "inputs1 = Input(shape=(4096,))\n",
        "fe1 = Dropout(0.4)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "# sequence feature layers\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.4)(se1)\n",
        "se3 = LSTM(256)(se2)\n",
        "\n",
        "# decoder model\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adagrad')\n",
        "\n",
        "# plot the model\n",
        "plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-19T23:55:22.268102Z",
          "iopub.execute_input": "2022-04-19T23:55:22.268887Z",
          "iopub.status.idle": "2022-04-19T23:55:26.166997Z",
          "shell.execute_reply.started": "2022-04-19T23:55:22.26885Z",
          "shell.execute_reply": "2022-04-19T23:55:26.166159Z"
        },
        "trusted": true,
        "id": "ecmbRNnTji4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Training (Commented)**\n",
        "\n",
        "The following can be executed! It will take time to run a 30 epoch model approximately: 30 Mins and since commented!\n",
        "\n",
        "P.S: We load the saved model below to show the predictions"
      ],
      "metadata": {
        "id": "7q6SRwKl9otO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # train the model\n",
        "# epochs = 20\n",
        "# batch_size = 32\n",
        "# steps = len(train) // batch_size\n",
        "\n",
        "# history_loss = []\n",
        "\n",
        "# for i in range(epochs):\n",
        "#     # create data generator\n",
        "#     generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n",
        "#     # fit for one epoch\n",
        "#     history = model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "#     history_loss.append(history.history['loss'])\n",
        "\n",
        "# # save the model\n",
        "# model.save(WORKING_DIR+'/LSTM_categorical_crossentropy_Adagrad.h5')\n",
        "\n",
        "# # Saving the loss\n",
        "# error = pd.DataFrame(history_loss)\n",
        "# error.to_csv('LSTM_categorical_crossentropy_Adagrad_loss.csv')\n"
      ],
      "metadata": {
        "id": "BoB7I_Mx9ni3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Hyperparamter for LSTM**\n",
        "We have changed the optimizer for LSTM we are using 'Adam' in the below model.\n",
        "We have also increased the epochs of the model"
      ],
      "metadata": {
        "id": "WPEq-Keq9cLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder model\n",
        "# image feature layers\n",
        "inputs1 = Input(shape=(4096,))\n",
        "fe1 = Dropout(0.4)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "# sequence feature layers\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.4)(se1)\n",
        "se3 = LSTM(256)(se2)\n",
        "\n",
        "# decoder model\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
        "\n",
        "# plot the model\n",
        "plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "gys9aHub9YKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # train the model\n",
        "# epochs = 30\n",
        "# batch_size = 32\n",
        "# steps = len(train) // batch_size\n",
        "\n",
        "# history_loss = []\n",
        "\n",
        "# for i in range(epochs):\n",
        "#     # create data generator\n",
        "#     generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n",
        "#     # fit for one epoch\n",
        "#     history = model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "#     history_loss.append(history.history['loss'])\n",
        "\n",
        "# # save the model\n",
        "# model.save(WORKING_DIR+'/LSTM_categorical_crossentropy_Adam.h5')\n",
        "\n",
        "# # Saving the loss\n",
        "# error = pd.DataFrame(history_loss)\n",
        "# error.to_csv('LSTM_categorical_crossentropy_Adam_loss.csv')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-19T23:55:33.15681Z",
          "iopub.execute_input": "2022-04-19T23:55:33.157494Z",
          "iopub.status.idle": "2022-04-20T00:13:44.044743Z",
          "shell.execute_reply.started": "2022-04-19T23:55:33.157447Z",
          "shell.execute_reply": "2022-04-20T00:13:44.043733Z"
        },
        "trusted": true,
        "id": "D8UJ9qjXji4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model 2: GRU Gates Recurrent Unit**"
      ],
      "metadata": {
        "id": "-sZgqYNEjhTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder model\n",
        "# image feature layers\n",
        "inputs1 = Input(shape=(4096,))\n",
        "fe1 = Dropout(0.4)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "# sequence feature layers\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.4)(se1)\n",
        "se3 = GRU(256)(se2)\n",
        "\n",
        "# decoder model\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
        "\n",
        "# plot the model\n",
        "plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "Q64y7CXOj3iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Training (Commented)**"
      ],
      "metadata": {
        "id": "9h-LzEeJjuxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # train the model\n",
        "# epochs = 1\n",
        "# batch_size = 32\n",
        "# steps = len(train) // batch_size\n",
        "\n",
        "# history_loss = []\n",
        "\n",
        "# for i in range(epochs):\n",
        "#     # create data generator\n",
        "#     generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n",
        "#     # fit for one epoch\n",
        "#     history = model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "#     history_loss.append(history.history['loss'])\n",
        "\n",
        "# # save the model\n",
        "# model.save(WORKING_DIR+'/GRU_categorical_crossentropy_Adam.h5')\n",
        "\n",
        "# # Saving the loss\n",
        "# error = pd.DataFrame(history_loss)\n",
        "# error.to_csv('GRU_categorical_crossentropy_Adagrad_Loss.csv')\n"
      ],
      "metadata": {
        "id": "aJsnPmdikJHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Hyperparamter for GRU**\n",
        "We have changed the optimizer for GRU we are using 'Adam' in the below model"
      ],
      "metadata": {
        "id": "P_McAGB4kDF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder model\n",
        "# image feature layers\n",
        "inputs1 = Input(shape=(4096,))\n",
        "fe1 = Dropout(0.4)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "# sequence feature layers\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.4)(se1)\n",
        "se3 = GRU(256)(se2)\n",
        "\n",
        "# decoder model\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Rmsprop')\n",
        "\n",
        "# plot the model\n",
        "plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "L9GeAMD0kYwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5) **Performance metric of the algorithm**\n",
        "\n",
        "Note: We have saved all the performance metrics so we are just loading it from our saved files and showing it\n",
        "\n"
      ],
      "metadata": {
        "id": "DqO-6G9kNjuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) BLEU Score Explaination:\n",
        "\n",
        "We are using BLEU score to judge the image captioning for the predicted images with actual captions to compare the model performance.\n",
        "\n",
        "**BLEU-n** is just the geometric average of the n-gram precision.\n",
        "\n",
        "```\n",
        "(precisely it's string matching, at different n-gram levels, between references and hypotheses; \n",
        "\n",
        "that's why there has been much criticism on this metric. \n",
        "\n",
        "But, people still use it anyways because it has stuck with the community for ages)\n",
        "```\n",
        "\n",
        "For example, **BLEU-1** is simply the **unigram precision**, **BLEU-2** is the **geometric average of unigram and bigram precision**, **BLEU-3** is the **geometric average of unigram, bigram, and trigram precision** and so on.\n",
        "\n",
        "Having said that, if you want to compute specific **n-gram BLEU scores**, you have to pass a weights parameter when you call **corpus_bleu** . Note that if you ignore passing this weights parameter, then by default BLEU-4 scores are returned, which is what happening in the evaluation here.\n",
        "\n",
        "To compute, BLEU-1 you can call copus_bleu with weights as\n",
        "```\n",
        "weights = (1.0/1.0, )\n",
        "corpus_bleu(references, hypotheses, weights)\n",
        "```\n",
        "\n",
        "To compute, BLEU-2 you can call corpus_bleu with weights as\n",
        "\n",
        "```\n",
        "weights=(1.0/2.0, 1.0/2.0,)\n",
        "corpus_bleu(references, hypotheses, weights)\n",
        "```\n",
        "\n",
        "To compute, BLEU-3 you can call corpus_bleu with weights as\n",
        "\n",
        "```\n",
        "weights=(1.0/3.0, 1.0/3.0, 1.0/3.0,)\n",
        "corpus_bleu(references, hypotheses, weights)\n",
        "```\n",
        "\n",
        "To compute, BLEU-5 you can call corpus_bleu with weights as\n",
        "\n",
        "```\n",
        "weights=(1.0/5.0, 1.0/5.0, 1.0/5.0, 1.0/5.0, 1.0/5.0,)\n",
        "corpus_bleu(references, hypotheses, weights)\n",
        "```\n"
      ],
      "metadata": {
        "id": "WSc-_9mR9lid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Bleu_Comprehensive = pd.read_csv(WORKING_DIR + 'Bleu_Comprehensive.csv')\n",
        "for i in range(1,5):\n",
        "  fig = px.bar(Bleu_Comprehensive, x='Algorithm', y='BLEU_'+str(i), color='BLEU_'+str(i), width = 600,height=400,title=\"Bleu Score Comparision for \"+ 'Bleu_'+str(i))\n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "opaRVtkUNhE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2) Loss VS Epoch**\n",
        "\n",
        "Explaination: epoch refers to the passing of whole dataset through the network; that will be counted as one epoch. So in the metric we will see how is the loss of the model in each epoch and get to know if we need to train our model for more epoch's"
      ],
      "metadata": {
        "id": "2ksCj9nJMrkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Loss_Comprehensive = pd.read_csv(WORKING_DIR + 'Loss_Comprehensive.csv')\n",
        "fig = px.line(Loss_Comprehensive,x = 'Epoch',y = 'Loss' , color='Algorithm' , title='Loss VS Epoch')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "ElXmzFPWIUYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6) Inference Pipeline**"
      ],
      "metadata": {
        "id": "iXzNzN6fZaVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "# validate with test data\n",
        "def idx_to_word(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "# generate caption for an image\n",
        "def predict_caption(model, image, tokenizer, max_length):\n",
        "    # add start tag for generation process\n",
        "    in_text = 'startseq'\n",
        "    # iterate over the max length of sequence\n",
        "    for i in range(max_length):\n",
        "        # encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # pad the sequence\n",
        "        sequence = pad_sequences([sequence], max_length)\n",
        "        # predict next word\n",
        "        yhat = model.predict([image, sequence], verbose=0)\n",
        "        # get index with high probability\n",
        "        yhat = np.argmax(yhat)\n",
        "        # convert index to word\n",
        "        word = idx_to_word(yhat, tokenizer)\n",
        "        # stop if word not found\n",
        "        if word is None:\n",
        "            break\n",
        "        # append word as input for generating next word\n",
        "        in_text += \" \" + word\n",
        "        # stop if we reach end tag\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text\n",
        "\n",
        "def calculate_bleu(model,test,tokenizer,max_length):\n",
        "  actual, predicted = list(), list()\n",
        "  for key in tqdm(test):\n",
        "      # get actual caption\n",
        "      captions = mapping[key]\n",
        "      # predict the caption for image\n",
        "      y_pred = predict_caption(model, features[key], tokenizer, max_length) \n",
        "      # split into words\n",
        "      actual_captions = [caption.split() for caption in captions]\n",
        "      y_pred = y_pred.split()\n",
        "      # append to the list\n",
        "      actual.append(actual_captions)\n",
        "      predicted.append(y_pred)\n",
        "  # calcuate BLEU score\n",
        "  print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "  print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "  print(\"BLEU-3: %f\" % corpus_bleu(actual, predicted, weights=(0.33, 0.33, 0.33, 0)))\n",
        "  print(\"BLEU-4: %f\" % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "def generate_caption(model,image_name):\n",
        "    image_id = image_name.split('.')[0]\n",
        "    img_path = os.path.join(BASE_DIR, \"Images\", image_name)\n",
        "    image = Image.open(img_path)\n",
        "    captions = mapping[image_id]\n",
        "    print('---------------------Actual---------------------')\n",
        "    for caption in captions:\n",
        "        print(caption)\n",
        "    # predict the caption\n",
        "    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n",
        "    print('--------------------Predicted--------------------')\n",
        "    print(y_pred)\n",
        "    plt.imshow(image)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-20T00:16:34.302084Z",
          "iopub.execute_input": "2022-04-20T00:16:34.302947Z",
          "iopub.status.idle": "2022-04-20T00:16:52.228445Z",
          "shell.execute_reply.started": "2022-04-20T00:16:34.302892Z",
          "shell.execute_reply": "2022-04-20T00:16:52.227339Z"
        },
        "trusted": true,
        "id": "WcpfqJW-ji4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "GRU_Rmsprop = keras.models.load_model(WORKING_DIR + 'VGG19_GRU_Rmsprop/GRU_Rmsprop.h5')\n",
        "GRU_Adam = keras.models.load_model(WORKING_DIR + 'VGG19_GRU_Adam/GRU_Adam.h5')\n",
        "LSTM_Adagrad = keras.models.load_model(WORKING_DIR + 'VGG19_LSTM_Adagrad/LSTM_Adagrad.h5')\n",
        "LSTM_Adam = keras.models.load_model(WORKING_DIR + 'VGG19_LSTM_Adam/LSTM_Adam.h5')\n",
        "print(\"All Models Loaded\")"
      ],
      "metadata": {
        "id": "0PK16D5vZfhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_caption(GRU_Rmsprop,\"1020651753_06077ec457.jpg\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-19T23:30:47.907923Z",
          "iopub.execute_input": "2022-04-19T23:30:47.910196Z",
          "iopub.status.idle": "2022-04-19T23:30:48.777641Z",
          "shell.execute_reply.started": "2022-04-19T23:30:47.910161Z",
          "shell.execute_reply": "2022-04-19T23:30:48.776939Z"
        },
        "trusted": true,
        "id": "LDmB-YsZji4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_caption(LSTM_Adagrad,\"1020651753_06077ec457.jpg\")"
      ],
      "metadata": {
        "id": "MBQOw6KpeagD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_caption(GRU_Adam,\"1020651753_06077ec457.jpg\")"
      ],
      "metadata": {
        "id": "b9CAeMfGefIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_caption(LSTM_Adam,\"1020651753_06077ec457.jpg\")"
      ],
      "metadata": {
        "id": "LTW9jYY8ehhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7) Conclusion\n",
        "\n",
        "In this notebook we tried to build a image captioning model. Following are some of our takeways:\n",
        "\n",
        "Model Variations:\n",
        "\n",
        "1.   LSTM + Adam (Optimizer) Epochs 10/25\n",
        "2.   LSTM + Adagrad (Optimizer) Epochs 10/25\n",
        "3.   GRU + Adam (Optimizer) Epochs 10/25\n",
        "4.   GRU + RMSProp (Optimizer) Epochs 10/25\n",
        "\n",
        "\n",
        "Based on these variations we evaluated their results based on the BLEU score metric and we found that:\n",
        "\n",
        "1) LSTM Adam performed well on 3 Variations of BLEU score (1-Gram,2-Gram,3-Gram)\n",
        "\n",
        "2) GRU RMSProp performed well on all 4 Variations of BLEU score (1-Gram,2-Gram,3-Gram,4-Gram)\n"
      ],
      "metadata": {
        "id": "nrmYtuPP_coU"
      }
    }
  ]
}