{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# In this  Notebook, I have Created Image Caption Generator using ResNet and Transformer Decoder Model. It can be divided into 2 steps :\n## **Step 1** : Create features for Images Using Resnet\n## **Step 2** : Train Transformer Decoder Model which predicts next word given a sequence of tokens and Image Features from Step1\n## Please do UpVote this notebook if you liked the content","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom collections import Counter \nimport torchvision\nfrom torchvision import transforms\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport math\nimport torch.nn.functional as F\nimport pickle\nimport gc\nimport random\npd.set_option('display.max_colwidth', None)\nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:56:32.162303Z","iopub.execute_input":"2022-04-17T20:56:32.162619Z","iopub.status.idle":"2022-04-17T20:56:32.170120Z","shell.execute_reply.started":"2022-04-17T20:56:32.162588Z","shell.execute_reply":"2022-04-17T20:56:32.169137Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Read Data.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/flickr8k/captions.txt\", sep=',')\nprint(len(df))\ndisplay(df.head(3))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:56:34.344338Z","iopub.execute_input":"2022-04-17T20:56:34.344670Z","iopub.status.idle":"2022-04-17T20:56:34.442396Z","shell.execute_reply.started":"2022-04-17T20:56:34.344621Z","shell.execute_reply":"2022-04-17T20:56:34.441570Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(len(df))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T22:02:22.443844Z","iopub.execute_input":"2022-04-17T22:02:22.444174Z","iopub.status.idle":"2022-04-17T22:02:22.449026Z","shell.execute_reply.started":"2022-04-17T22:02:22.444142Z","shell.execute_reply":"2022-04-17T22:02:22.447943Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing -> Remove Single Character and non alpha Words. Add <Start>, <end> and <pad> tokens. <pad> token is appended such that length in max_seq_len (maximum length across all captions which is 33 in our case)  ","metadata":{}},{"cell_type":"code","source":"def remove_single_char_word(word_list):\n    lst = []\n    for word in word_list:\n        if len(word)>1:\n            lst.append(word)\n\n    return lst","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:56:41.633608Z","iopub.execute_input":"2022-04-17T20:56:41.633957Z","iopub.status.idle":"2022-04-17T20:56:41.638081Z","shell.execute_reply.started":"2022-04-17T20:56:41.633926Z","shell.execute_reply":"2022-04-17T20:56:41.637296Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df['cleaned_caption'] = df['caption'].apply(lambda caption : ['<start>'] + [word.lower() if word.isalpha() else '' for word in caption.split(\" \")] + ['<end>'])\ndf['cleaned_caption']  = df['cleaned_caption'].apply(lambda x : remove_single_char_word(x))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:56:44.083093Z","iopub.execute_input":"2022-04-17T20:56:44.083416Z","iopub.status.idle":"2022-04-17T20:56:44.461475Z","shell.execute_reply.started":"2022-04-17T20:56:44.083387Z","shell.execute_reply":"2022-04-17T20:56:44.460589Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df['seq_len'] = df['cleaned_caption'].apply(lambda x : len(x))\nmax_seq_len = df['seq_len'].max()\nprint(max_seq_len)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:56:46.184724Z","iopub.execute_input":"2022-04-17T20:56:46.185076Z","iopub.status.idle":"2022-04-17T20:56:46.237534Z","shell.execute_reply.started":"2022-04-17T20:56:46.185045Z","shell.execute_reply":"2022-04-17T20:56:46.236645Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df.drop(['seq_len'], axis = 1, inplace = True)\ndf['cleaned_caption'] = df['cleaned_caption'].apply(lambda caption : caption + ['<pad>']*(max_seq_len-len(caption)) )","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:56:48.179116Z","iopub.execute_input":"2022-04-17T20:56:48.179443Z","iopub.status.idle":"2022-04-17T20:56:48.274314Z","shell.execute_reply.started":"2022-04-17T20:56:48.179410Z","shell.execute_reply":"2022-04-17T20:56:48.273417Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"display(df.head(2))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:56:49.813056Z","iopub.execute_input":"2022-04-17T20:56:49.813449Z","iopub.status.idle":"2022-04-17T20:56:49.824886Z","shell.execute_reply.started":"2022-04-17T20:56:49.813409Z","shell.execute_reply":"2022-04-17T20:56:49.823260Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Create Vocab and mapping of token to ID","metadata":{}},{"cell_type":"code","source":"word_list = df['cleaned_caption'].apply(lambda x : \" \".join(x)).str.cat(sep = ' ').split(' ')\nword_dict = Counter(word_list)\nword_dict =  sorted(word_dict, key=word_dict.get, reverse=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:56:55.408251Z","iopub.execute_input":"2022-04-17T20:56:55.408578Z","iopub.status.idle":"2022-04-17T20:56:55.767102Z","shell.execute_reply.started":"2022-04-17T20:56:55.408547Z","shell.execute_reply":"2022-04-17T20:56:55.766098Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print(len(word_dict))\nprint(word_dict[:5])","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:56:58.379177Z","iopub.execute_input":"2022-04-17T20:56:58.379542Z","iopub.status.idle":"2022-04-17T20:56:58.385147Z","shell.execute_reply.started":"2022-04-17T20:56:58.379503Z","shell.execute_reply":"2022-04-17T20:56:58.383067Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Vocab size is 8360","metadata":{}},{"cell_type":"code","source":"vocab_size = len(word_dict)\nprint(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:57:02.279546Z","iopub.execute_input":"2022-04-17T20:57:02.279908Z","iopub.status.idle":"2022-04-17T20:57:02.287037Z","shell.execute_reply.started":"2022-04-17T20:57:02.279877Z","shell.execute_reply":"2022-04-17T20:57:02.286230Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"index_to_word = {index: word for index, word in enumerate(word_dict)}\nword_to_index = {word: index for index, word in enumerate(word_dict)}\nprint(len(index_to_word), len(word_to_index))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:57:04.669728Z","iopub.execute_input":"2022-04-17T20:57:04.670049Z","iopub.status.idle":"2022-04-17T20:57:04.680781Z","shell.execute_reply.started":"2022-04-17T20:57:04.670018Z","shell.execute_reply":"2022-04-17T20:57:04.679759Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Covert sequence of tokens to IDs","metadata":{}},{"cell_type":"code","source":"df['text_seq']  = df['cleaned_caption'].apply(lambda caption : [word_to_index[word] for word in caption] )","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:57:10.645360Z","iopub.execute_input":"2022-04-17T20:57:10.645708Z","iopub.status.idle":"2022-04-17T20:57:10.968784Z","shell.execute_reply.started":"2022-04-17T20:57:10.645666Z","shell.execute_reply":"2022-04-17T20:57:10.967917Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"display(df.head(2))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:57:12.424156Z","iopub.execute_input":"2022-04-17T20:57:12.424489Z","iopub.status.idle":"2022-04-17T20:57:12.437916Z","shell.execute_reply.started":"2022-04-17T20:57:12.424459Z","shell.execute_reply":"2022-04-17T20:57:12.436869Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split In Train and validation data. Same Image should not be present in both training and validation data ","metadata":{}},{"cell_type":"code","source":"df = df.sort_values(by = 'image')\ntrain = df.iloc[:int(0.9*len(df))]\nvalid = df.iloc[int(0.9*len(df)):]","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:57:15.778805Z","iopub.execute_input":"2022-04-17T20:57:15.779118Z","iopub.status.idle":"2022-04-17T20:57:15.829149Z","shell.execute_reply.started":"2022-04-17T20:57:15.779088Z","shell.execute_reply":"2022-04-17T20:57:15.828325Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"print(len(train), train['image'].nunique())\nprint(len(valid), valid['image'].nunique())","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:57:17.358697Z","iopub.execute_input":"2022-04-17T20:57:17.359013Z","iopub.status.idle":"2022-04-17T20:57:17.370869Z","shell.execute_reply.started":"2022-04-17T20:57:17.358986Z","shell.execute_reply":"2022-04-17T20:57:17.369848Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extract features from Images Using Resnet","metadata":{}},{"cell_type":"code","source":"train_samples = len(train)\nprint(train_samples)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:57:19.630055Z","iopub.execute_input":"2022-04-17T20:57:19.630418Z","iopub.status.idle":"2022-04-17T20:57:19.636904Z","shell.execute_reply.started":"2022-04-17T20:57:19.630380Z","shell.execute_reply":"2022-04-17T20:57:19.635688Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"unq_train_imgs = train[['image']].drop_duplicates()\nunq_valid_imgs = valid[['image']].drop_duplicates()\nprint(len(unq_train_imgs), len(unq_valid_imgs))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:57:21.594851Z","iopub.execute_input":"2022-04-17T20:57:21.595168Z","iopub.status.idle":"2022-04-17T20:57:21.610907Z","shell.execute_reply.started":"2022-04-17T20:57:21.595139Z","shell.execute_reply":"2022-04-17T20:57:21.609924Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:57:24.679649Z","iopub.execute_input":"2022-04-17T20:57:24.680153Z","iopub.status.idle":"2022-04-17T20:57:24.775289Z","shell.execute_reply.started":"2022-04-17T20:57:24.680115Z","shell.execute_reply":"2022-04-17T20:57:24.774112Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class extractImageFeatureResNetDataSet():\n    def __init__(self, data):\n        self.data = data \n        self.scaler = transforms.Resize([224, 224])\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n        self.to_tensor = transforms.ToTensor()\n    def __len__(self):  \n        return len(self.data)\n\n    def __getitem__(self, idx):\n\n        image_name = self.data.iloc[idx]['image']\n        img_loc = '../input/flickr8k/Images/'+str(image_name)\n\n        img = Image.open(img_loc)\n        t_img = self.normalize(self.to_tensor(self.scaler(img)))\n\n        return image_name, t_img","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:57:28.296170Z","iopub.execute_input":"2022-04-17T20:57:28.296516Z","iopub.status.idle":"2022-04-17T20:57:28.303409Z","shell.execute_reply.started":"2022-04-17T20:57:28.296484Z","shell.execute_reply":"2022-04-17T20:57:28.302384Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"train_ImageDataset_ResNet = extractImageFeatureResNetDataSet(unq_train_imgs)\ntrain_ImageDataloader_ResNet = DataLoader(train_ImageDataset_ResNet, batch_size = 1, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:57:30.930953Z","iopub.execute_input":"2022-04-17T20:57:30.931295Z","iopub.status.idle":"2022-04-17T20:57:30.936449Z","shell.execute_reply.started":"2022-04-17T20:57:30.931264Z","shell.execute_reply":"2022-04-17T20:57:30.935464Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"valid_ImageDataset_ResNet = extractImageFeatureResNetDataSet(unq_valid_imgs)\nvalid_ImageDataloader_ResNet = DataLoader(valid_ImageDataset_ResNet, batch_size = 1, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:57:32.977469Z","iopub.execute_input":"2022-04-17T20:57:32.978144Z","iopub.status.idle":"2022-04-17T20:57:32.982990Z","shell.execute_reply.started":"2022-04-17T20:57:32.978100Z","shell.execute_reply":"2022-04-17T20:57:32.982059Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"resnet18 = torchvision.models.resnet18(pretrained=True).to(device)\nresnet18.eval()\nlist(resnet18._modules)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:57:35.171422Z","iopub.execute_input":"2022-04-17T20:57:35.171748Z","iopub.status.idle":"2022-04-17T20:57:39.955352Z","shell.execute_reply.started":"2022-04-17T20:57:35.171717Z","shell.execute_reply":"2022-04-17T20:57:39.954522Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"resNet18Layer4 = resnet18._modules.get('layer4').to(device)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:57:41.769127Z","iopub.execute_input":"2022-04-17T20:57:41.769448Z","iopub.status.idle":"2022-04-17T20:57:41.775576Z","shell.execute_reply.started":"2022-04-17T20:57:41.769417Z","shell.execute_reply":"2022-04-17T20:57:41.774520Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def get_vector(t_img):\n    \n    t_img = Variable(t_img)\n    my_embedding = torch.zeros(1, 512, 7, 7)\n    def copy_data(m, i, o):\n        my_embedding.copy_(o.data)\n    \n    h = resNet18Layer4.register_forward_hook(copy_data)\n    resnet18(t_img)\n    \n    h.remove()\n    return my_embedding","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:57:43.711221Z","iopub.execute_input":"2022-04-17T20:57:43.711537Z","iopub.status.idle":"2022-04-17T20:57:43.716813Z","shell.execute_reply.started":"2022-04-17T20:57:43.711506Z","shell.execute_reply":"2022-04-17T20:57:43.715818Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"extract_imgFtr_ResNet_train = {}\nfor image_name, t_img in tqdm(train_ImageDataloader_ResNet):\n    t_img = t_img.to(device)\n    embdg = get_vector(t_img)\n    \n    extract_imgFtr_ResNet_train[image_name[0]] = embdg\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-17T20:57:46.449434Z","iopub.execute_input":"2022-04-17T20:57:46.449795Z","iopub.status.idle":"2022-04-17T21:00:31.653641Z","shell.execute_reply.started":"2022-04-17T20:57:46.449763Z","shell.execute_reply":"2022-04-17T21:00:31.652727Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"a_file = open(\"./EncodedImageTrainResNet.pkl\", \"wb\")\npickle.dump(extract_imgFtr_ResNet_train, a_file)\na_file.close()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:00:53.271581Z","iopub.execute_input":"2022-04-17T21:00:53.271929Z","iopub.status.idle":"2022-04-17T21:00:54.729247Z","shell.execute_reply.started":"2022-04-17T21:00:53.271899Z","shell.execute_reply":"2022-04-17T21:00:54.728414Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"extract_imgFtr_ResNet_valid = {}\nfor image_name, t_img in tqdm(valid_ImageDataloader_ResNet):\n    t_img = t_img.to(device)\n    embdg = get_vector(t_img)\n \n    extract_imgFtr_ResNet_valid[image_name[0]] = embdg","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:01:04.987391Z","iopub.execute_input":"2022-04-17T21:01:04.987747Z","iopub.status.idle":"2022-04-17T21:01:23.949096Z","shell.execute_reply.started":"2022-04-17T21:01:04.987711Z","shell.execute_reply":"2022-04-17T21:01:23.948216Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"a_file = open(\"./EncodedImageValidResNet.pkl\", \"wb\")\npickle.dump(extract_imgFtr_ResNet_valid, a_file)\na_file.close()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:01:25.588571Z","iopub.execute_input":"2022-04-17T21:01:25.588940Z","iopub.status.idle":"2022-04-17T21:01:25.755789Z","shell.execute_reply.started":"2022-04-17T21:01:25.588908Z","shell.execute_reply":"2022-04-17T21:01:25.754933Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"## Create DataLoader which will be used to load data into Transformer Model.\n## FlickerDataSetResnet will return caption sequence, 1 timestep left shifted caption sequence which model will predict and Stored Image features from ResNet.","metadata":{}},{"cell_type":"code","source":"class FlickerDataSetResnet():\n    def __init__(self, data, pkl_file):\n        self.data = data\n        self.encodedImgs = pd.read_pickle(pkl_file)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n    \n        caption_seq = self.data.iloc[idx]['text_seq']\n        target_seq = caption_seq[1:]+[0]\n\n        image_name = self.data.iloc[idx]['image']\n        image_tensor = self.encodedImgs[image_name]\n        image_tensor = image_tensor.permute(0,2,3,1)\n        image_tensor_view = image_tensor.view(image_tensor.size(0), -1, image_tensor.size(3))\n\n        return torch.tensor(caption_seq), torch.tensor(target_seq), image_tensor_view","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:01:31.454063Z","iopub.execute_input":"2022-04-17T21:01:31.454380Z","iopub.status.idle":"2022-04-17T21:01:31.461211Z","shell.execute_reply.started":"2022-04-17T21:01:31.454351Z","shell.execute_reply":"2022-04-17T21:01:31.460184Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"train_dataset_resnet = FlickerDataSetResnet(train, 'EncodedImageTrainResNet.pkl')\ntrain_dataloader_resnet = DataLoader(train_dataset_resnet, batch_size = 32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:01:34.163306Z","iopub.execute_input":"2022-04-17T21:01:34.163658Z","iopub.status.idle":"2022-04-17T21:01:35.913152Z","shell.execute_reply.started":"2022-04-17T21:01:34.163610Z","shell.execute_reply":"2022-04-17T21:01:35.912242Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"valid_dataset_resnet = FlickerDataSetResnet(valid, 'EncodedImageValidResNet.pkl')\nvalid_dataloader_resnet = DataLoader(valid_dataset_resnet, batch_size = 32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:01:36.728375Z","iopub.execute_input":"2022-04-17T21:01:36.728726Z","iopub.status.idle":"2022-04-17T21:01:36.827440Z","shell.execute_reply.started":"2022-04-17T21:01:36.728694Z","shell.execute_reply":"2022-04-17T21:01:36.826575Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"## Create Transformer Decoder Model. This Model will take caption sequence and the extracted resnet image features as input and ouput 1 timestep shifted (left) caption sequence. \n## In the Transformer decoder, lookAhead and padding mask has also been applied","metadata":{}},{"cell_type":"markdown","source":"### Position Embedding","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=max_seq_len):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        \n\n    def forward(self, x):\n        if self.pe.size(0) < x.size(0):\n            self.pe = self.pe.repeat(x.size(0), 1, 1).to(device)\n        self.pe = self.pe[:x.size(0), : , : ]\n        \n        x = x + self.pe\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:01:39.971534Z","iopub.execute_input":"2022-04-17T21:01:39.971919Z","iopub.status.idle":"2022-04-17T21:01:39.980993Z","shell.execute_reply.started":"2022-04-17T21:01:39.971886Z","shell.execute_reply":"2022-04-17T21:01:39.980048Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## Transformer Decoder","metadata":{}},{"cell_type":"code","source":"class ImageCaptionModel(nn.Module):\n    def __init__(self, n_head, n_decoder_layer, vocab_size, embedding_size):\n        super(ImageCaptionModel, self).__init__()\n        self.pos_encoder = PositionalEncoding(embedding_size, 0.1)\n        self.TransformerDecoderLayer = nn.TransformerDecoderLayer(d_model =  embedding_size, nhead = n_head)\n        self.TransformerDecoder = nn.TransformerDecoder(decoder_layer = self.TransformerDecoderLayer, num_layers = n_decoder_layer)\n        self.embedding_size = embedding_size\n        self.embedding = nn.Embedding(vocab_size , embedding_size)\n        self.last_linear_layer = nn.Linear(embedding_size, vocab_size)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.embedding.weight.data.uniform_(-initrange, initrange)\n        self.last_linear_layer.bias.data.zero_()\n        self.last_linear_layer.weight.data.uniform_(-initrange, initrange)\n\n    def generate_Mask(self, size, decoder_inp):\n        decoder_input_mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n        decoder_input_mask = decoder_input_mask.float().masked_fill(decoder_input_mask == 0, float('-inf')).masked_fill(decoder_input_mask == 1, float(0.0))\n\n        decoder_input_pad_mask = decoder_inp.float().masked_fill(decoder_inp == 0, float(0.0)).masked_fill(decoder_inp > 0, float(1.0))\n        decoder_input_pad_mask_bool = decoder_inp == 0\n\n        return decoder_input_mask, decoder_input_pad_mask, decoder_input_pad_mask_bool\n\n    def forward(self, encoded_image, decoder_inp):\n        encoded_image = encoded_image.permute(1,0,2)\n        \n\n        decoder_inp_embed = self.embedding(decoder_inp)* math.sqrt(self.embedding_size)\n        \n        decoder_inp_embed = self.pos_encoder(decoder_inp_embed)\n        decoder_inp_embed = decoder_inp_embed.permute(1,0,2)\n        \n\n        decoder_input_mask, decoder_input_pad_mask, decoder_input_pad_mask_bool = self.generate_Mask(decoder_inp.size(1), decoder_inp)\n        decoder_input_mask = decoder_input_mask.to(device)\n        decoder_input_pad_mask = decoder_input_pad_mask.to(device)\n        decoder_input_pad_mask_bool = decoder_input_pad_mask_bool.to(device)\n        \n\n        decoder_output = self.TransformerDecoder(tgt = decoder_inp_embed, memory = encoded_image, tgt_mask = decoder_input_mask, tgt_key_padding_mask = decoder_input_pad_mask_bool)\n        \n        final_output = self.last_linear_layer(decoder_output)\n\n        return final_output,  decoder_input_pad_mask\n","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:01:51.680519Z","iopub.execute_input":"2022-04-17T21:01:51.680974Z","iopub.status.idle":"2022-04-17T21:01:51.693086Z","shell.execute_reply.started":"2022-04-17T21:01:51.680929Z","shell.execute_reply":"2022-04-17T21:01:51.692248Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Train the Model","metadata":{}},{"cell_type":"markdown","source":"### The cross entropy loss has been masked at time steps where input token is <'pad'>.","metadata":{}},{"cell_type":"code","source":"EPOCH = 5","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:30:04.078992Z","iopub.execute_input":"2022-04-17T21:30:04.079303Z","iopub.status.idle":"2022-04-17T21:30:04.083184Z","shell.execute_reply.started":"2022-04-17T21:30:04.079273Z","shell.execute_reply":"2022-04-17T21:30:04.082174Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"ictModel = ImageCaptionModel(16, 4, vocab_size, 512).to(device)\noptimizer = torch.optim.Adam(ictModel.parameters(), lr = 0.00001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.8, patience=2, verbose = True)\ncriterion = torch.nn.CrossEntropyLoss(reduction='none')\nmin_val_loss = np.float('Inf')","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:28:03.586375Z","iopub.execute_input":"2022-04-17T21:28:03.586716Z","iopub.status.idle":"2022-04-17T21:28:03.866299Z","shell.execute_reply.started":"2022-04-17T21:28:03.586684Z","shell.execute_reply":"2022-04-17T21:28:03.865463Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"for epoch in tqdm(range(EPOCH)):\n    total_epoch_train_loss = 0\n    total_epoch_valid_loss = 0\n    total_train_words = 0\n    total_valid_words = 0\n    ictModel.train()\n\n    ### Train Loop\n    for caption_seq, target_seq, image_embed in train_dataloader_resnet:\n\n        optimizer.zero_grad()\n\n        image_embed = image_embed.squeeze(1).to(device)\n        caption_seq = caption_seq.to(device)\n        target_seq = target_seq.to(device)\n\n        output, padding_mask = ictModel.forward(image_embed, caption_seq)\n        output = output.permute(1, 2, 0)\n\n        loss = criterion(output,target_seq)\n\n        loss_masked = torch.mul(loss, padding_mask)\n\n        final_batch_loss = torch.sum(loss_masked)/torch.sum(padding_mask)\n\n        final_batch_loss.backward()\n        optimizer.step()\n        total_epoch_train_loss += torch.sum(loss_masked).detach().item()\n        total_train_words += torch.sum(padding_mask)\n\n \n    total_epoch_train_loss = total_epoch_train_loss/total_train_words\n  \n\n    ### Eval Loop\n    ictModel.eval()\n    with torch.no_grad():\n        for caption_seq, target_seq, image_embed in valid_dataloader_resnet:\n\n            image_embed = image_embed.squeeze(1).to(device)\n            caption_seq = caption_seq.to(device)\n            target_seq = target_seq.to(device)\n\n            output, padding_mask = ictModel.forward(image_embed, caption_seq)\n            output = output.permute(1, 2, 0)\n\n            loss = criterion(output,target_seq)\n\n            loss_masked = torch.mul(loss, padding_mask)\n\n            total_epoch_valid_loss += torch.sum(loss_masked).detach().item()\n            total_valid_words += torch.sum(padding_mask)\n\n    total_epoch_valid_loss = total_epoch_valid_loss/total_valid_words\n  \n    print(\"Epoch -> \", epoch,\" Training Loss -> \", total_epoch_train_loss.item(), \"Eval Loss -> \", total_epoch_valid_loss.item() )\n  \n    if min_val_loss > total_epoch_valid_loss:\n        print(\"Writing Model at epoch \", epoch)\n        torch.save(ictModel, './BestModel')\n        min_val_loss = total_epoch_valid_loss\n  \n\n    scheduler.step(total_epoch_valid_loss.item())\n","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:30:06.869988Z","iopub.execute_input":"2022-04-17T21:30:06.870387Z","iopub.status.idle":"2022-04-17T21:35:56.638300Z","shell.execute_reply.started":"2022-04-17T21:30:06.870346Z","shell.execute_reply":"2022-04-17T21:35:56.637485Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lets Generate Captions !!!","metadata":{}},{"cell_type":"code","source":"model = torch.load('./BestModel')\nstart_token = word_to_index['<start>']\nend_token = word_to_index['<end>']\npad_token = word_to_index['<pad>']\nmax_seq_len = 33\nprint(start_token, end_token, pad_token)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:38:52.839506Z","iopub.execute_input":"2022-04-17T21:38:52.839889Z","iopub.status.idle":"2022-04-17T21:38:52.963842Z","shell.execute_reply.started":"2022-04-17T21:38:52.839856Z","shell.execute_reply":"2022-04-17T21:38:52.963017Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"valid_img_embed = pd.read_pickle('EncodedImageValidResNet.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:38:55.277212Z","iopub.execute_input":"2022-04-17T21:38:55.277558Z","iopub.status.idle":"2022-04-17T21:38:55.378445Z","shell.execute_reply.started":"2022-04-17T21:38:55.277525Z","shell.execute_reply":"2022-04-17T21:38:55.377623Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"### Here in the below function,we are generating caption in beam search. K defines the topK token to look at each time step","metadata":{}},{"cell_type":"code","source":"def generate_caption(K, img_nm): \n    img_loc = '../input/flickr8k/Images/'+str(img_nm)\n    image = Image.open(img_loc).convert(\"RGB\")\n    plt.imshow(image)\n\n    model.eval() \n    valid_img_df = valid[valid['image']==img_nm]\n    print(\"Actual Caption : \")\n    print(valid_img_df['caption'].tolist())\n    img_embed = valid_img_embed[img_nm].to(device)\n\n\n    img_embed = img_embed.permute(0,2,3,1)\n    img_embed = img_embed.view(img_embed.size(0), -1, img_embed.size(3))\n\n\n    input_seq = [pad_token]*max_seq_len\n    input_seq[0] = start_token\n\n    input_seq = torch.tensor(input_seq).unsqueeze(0).to(device)\n    predicted_sentence = []\n    with torch.no_grad():\n        for eval_iter in range(0, max_seq_len):\n\n            output, padding_mask = model.forward(img_embed, input_seq)\n\n            output = output[eval_iter, 0, :]\n\n            values = torch.topk(output, K).values.tolist()\n            indices = torch.topk(output, K).indices.tolist()\n\n            next_word_index = random.choices(indices, values, k = 1)[0]\n\n            next_word = index_to_word[next_word_index]\n\n            input_seq[:, eval_iter+1] = next_word_index\n\n\n            if next_word == '<end>' :\n                break\n\n            predicted_sentence.append(next_word)\n    print(\"\\n\")\n    print(\"Predicted caption : \")\n    print(\" \".join(predicted_sentence+['.']))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:38:57.201902Z","iopub.execute_input":"2022-04-17T21:38:57.202211Z","iopub.status.idle":"2022-04-17T21:38:57.211908Z","shell.execute_reply.started":"2022-04-17T21:38:57.202181Z","shell.execute_reply":"2022-04-17T21:38:57.211111Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"### 1st Example ","metadata":{}},{"cell_type":"code","source":"generate_caption(1, unq_valid_imgs.iloc[50]['image'])","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:39:01.918318Z","iopub.execute_input":"2022-04-17T21:39:01.918640Z","iopub.status.idle":"2022-04-17T21:39:02.173114Z","shell.execute_reply.started":"2022-04-17T21:39:01.918601Z","shell.execute_reply":"2022-04-17T21:39:02.172325Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"generate_caption(2, unq_valid_imgs.iloc[50]['image'])","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:39:13.951195Z","iopub.execute_input":"2022-04-17T21:39:13.951536Z","iopub.status.idle":"2022-04-17T21:39:14.207501Z","shell.execute_reply.started":"2022-04-17T21:39:13.951504Z","shell.execute_reply":"2022-04-17T21:39:14.206727Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"### 2nd Example","metadata":{}},{"cell_type":"code","source":"generate_caption(1, unq_valid_imgs.iloc[100]['image'])","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:39:22.511139Z","iopub.execute_input":"2022-04-17T21:39:22.511459Z","iopub.status.idle":"2022-04-17T21:39:22.734904Z","shell.execute_reply.started":"2022-04-17T21:39:22.511428Z","shell.execute_reply":"2022-04-17T21:39:22.734063Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"generate_caption(2, unq_valid_imgs.iloc[100]['image'])","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:39:29.970070Z","iopub.execute_input":"2022-04-17T21:39:29.970393Z","iopub.status.idle":"2022-04-17T21:39:30.214766Z","shell.execute_reply.started":"2022-04-17T21:39:29.970364Z","shell.execute_reply":"2022-04-17T21:39:30.213887Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"### 3rd Example","metadata":{}},{"cell_type":"code","source":"generate_caption(1, unq_valid_imgs.iloc[500]['image'])","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:48:04.584172Z","iopub.execute_input":"2022-04-17T21:48:04.584537Z","iopub.status.idle":"2022-04-17T21:48:04.816363Z","shell.execute_reply.started":"2022-04-17T21:48:04.584504Z","shell.execute_reply":"2022-04-17T21:48:04.815573Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"generate_caption(2, unq_valid_imgs.iloc[500]['image'])","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:48:10.102193Z","iopub.execute_input":"2022-04-17T21:48:10.102526Z","iopub.status.idle":"2022-04-17T21:48:10.337208Z","shell.execute_reply.started":"2022-04-17T21:48:10.102495Z","shell.execute_reply":"2022-04-17T21:48:10.336353Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"### 4rth Example","metadata":{}},{"cell_type":"code","source":"generate_caption(1, unq_valid_imgs.iloc[600]['image'])","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:48:17.087186Z","iopub.execute_input":"2022-04-17T21:48:17.087539Z","iopub.status.idle":"2022-04-17T21:48:17.443518Z","shell.execute_reply.started":"2022-04-17T21:48:17.087506Z","shell.execute_reply":"2022-04-17T21:48:17.442561Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"generate_caption(2, unq_valid_imgs.iloc[600]['image'])","metadata":{"execution":{"iopub.status.busy":"2022-04-17T21:48:25.108261Z","iopub.execute_input":"2022-04-17T21:48:25.108584Z","iopub.status.idle":"2022-04-17T21:48:25.326791Z","shell.execute_reply.started":"2022-04-17T21:48:25.108554Z","shell.execute_reply":"2022-04-17T21:48:25.325963Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"## Thanks for going through the whole work. Please do upvote the notebook if you liked it.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}